{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.5 64-bit"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "interpreter": {
            "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Bag of Words + Logistic Regression"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Importing Libraries"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "source": [
                "import pandas as pd\r\n",
                "import re\r\n",
                "import nltk"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Importing the dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "dataset_train = pd.read_csv(r'data/reddit-train.csv')\r\n",
                "dataset_test = pd.read_csv('data/reddit-test.csv')\r\n",
                "frames = [dataset_train, dataset_test]\r\n",
                "dataset = pd.concat(frames)\r\n",
                "\r\n",
                "# Select text and label columns\r\n",
                "text = dataset[\"text\"]\r\n",
                "label = dataset[\"label\"]\r\n",
                "data_dict = {'text': text, 'label': label}\r\n",
                "dataset = pd.DataFrame(data_dict)\r\n",
                "print(dataset)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "                                                  text  label\n",
                        "0    He said he had not felt that way before, sugge...      1\n",
                        "1    Hey there r/assistance, Not sure if this is th...      0\n",
                        "2    My mom then hit me with the newspaper and it s...      1\n",
                        "3    until i met my new boyfriend, he is amazing, h...      1\n",
                        "4    October is Domestic Violence Awareness Month a...      1\n",
                        "..                                                 ...    ...\n",
                        "710  i have horrible vivid nightmares every night. ...      1\n",
                        "711  Also I can't think about both of them without ...      1\n",
                        "712  Furthermore, I told him before we got really s...      1\n",
                        "713  Here's the link to my amazon wish list where t...      0\n",
                        "714  How can I keep us protected? They have already...      1\n",
                        "\n",
                        "[3553 rows x 2 columns]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Set up dictionary functions"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "nltk.download('punkt')\r\n",
                "from nltk.tokenize import word_tokenize as wt\r\n",
                "\r\n",
                "nltk.download('stopwords')\r\n",
                "from nltk.corpus import stopwords\r\n",
                "\r\n",
                "from nltk.stem.porter import PorterStemmer\r\n",
                "stemmer = PorterStemmer()\r\n",
                "\r\n",
                "from autocorrect import Speller\r\n",
                "spell = Speller()"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Extract Features from Text"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "data = []\r\n",
                "for i in range(dataset.shape[0]):\r\n",
                "    sms = dataset.iloc[i, 0]\r\n",
                "    # remove non alphabatic characters\r\n",
                "    sms = re.sub('[^A-Za-z]', ' ', sms)\r\n",
                "    # make words lowercase, because Go and go will be considered as two words\r\n",
                "    sms = sms.lower()\r\n",
                "    # tokenising\r\n",
                "    tokenized_sms = wt(sms)\r\n",
                "    # remove stop words and stemming\r\n",
                "    sms_processed = []\r\n",
                "    for word in tokenized_sms:\r\n",
                "        if word not in set(stopwords.words('english')):\r\n",
                "            # sms_processed.append(stemmer.stem(word))\r\n",
                "            sms_processed.append(spell(stemmer.stem(word)))\r\n",
                "    sms_text = \" \".join(sms_processed)\r\n",
                "    data.append(sms_text)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Creating the Feature Matrix"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer\r\n",
                "matrix = CountVectorizer(max_features=1000)\r\n",
                "X = matrix.fit_transform(data).toarray()\r\n",
                "y = dataset.iloc[:, 1]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Split train and test data"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.model_selection import train_test_split\r\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Train Logistic Regression Model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.linear_model import LogisticRegression\r\n",
                "classifier = LogisticRegression(max_iter = 100)\r\n",
                "classifier.fit(X_train, y_train)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Predict test set results"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# predict class\r\n",
                "y_pred = classifier.predict(X_test)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Find Accuracy using Confusion matrix\r\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\r\n",
                "cm = confusion_matrix(y_test, y_pred)\r\n",
                "cr = classification_report(y_test, y_pred)\r\n",
                "\r\n",
                "accuracy = accuracy_score(y_test, y_pred)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "print(accuracy)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "0.7271448663853727\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ]
}