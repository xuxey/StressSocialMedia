{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Logistic Regression Classifier"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Importing Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Combining training files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_train = pd.read_csv(r'data/reddit-train.csv')\n",
                "dataset_test = pd.read_csv('data/reddit-test.csv')\n",
                "frames = [dataset_train, dataset_test]\n",
                "dataset = pd.concat(frames)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Isolating Independent \u0026 Dependent Variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = dataset.iloc[:, 7:]\n",
                "subreddits = np.array(dataset.iloc[:, 0])\n",
                "X['subreddits'] = subreddits\n",
                "y = dataset.iloc[:, dataset.columns == 'label'].values"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Encoding subreddits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\n",
                "X = np.array(ct.fit_transform(X))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Splitting testing and training data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Scaling data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "sc = StandardScaler()\n",
                "X_train = sc.fit_transform(X_train)\n",
                "X_test = sc.fit_transform(X_test)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training the Logistic Regression Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "/usr/lib/python3.9/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(*args, **kwargs)\n"
                },
                {
                    "data": {
                        "text/plain": "LogisticRegression()"
                    },
                    "execution_count": 61,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "classifier = LogisticRegression(max_iter = 100)\n",
                "classifier.fit(X_train, y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Predicting Test Set Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[1 1]\n [0 1]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [1 0]\n [0 0]\n [1 1]\n [1 1]\n [0 1]\n [0 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 0]\n [0 0]\n [0 0]\n [1 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [0 1]\n [1 1]\n [0 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [1 0]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 1]\n [0 0]\n [1 1]\n [1 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 0]\n [0 0]\n [1 1]\n [1 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [1 1]\n [0 1]\n [0 1]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 1]\n [1 1]\n [1 1]\n [1 0]]\n"
                }
            ],
            "source": [
                "y_pred = classifier.predict(X_test)\n",
                "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)[:100])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Checking Accuracy using Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[261  96]\n [ 65 289]]\n"
                },
                {
                    "data": {
                        "text/plain": "0.7735583684950773"
                    },
                    "execution_count": 63,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from sklearn.metrics import confusion_matrix, accuracy_score\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "print(cm)\n",
                "accuracy_score(y_test, y_pred)"
            ]
        }
    ]
}
